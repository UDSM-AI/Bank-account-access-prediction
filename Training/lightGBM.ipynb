{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48S6RVt_jXwp"
      },
      "source": [
        "# Financial Inclusion in Africa Starter Notebook\n",
        "\n",
        "\n",
        "This is a simple starter notebook to get started with the Financial Inclusion Competition on Zindi.\n",
        "\n",
        "This notebook covers:\n",
        "- Loading the data\n",
        "- Simple EDA and an example of feature enginnering\n",
        "- Data preprocessing and data wrangling\n",
        "- Creating a simple model\n",
        "- Making a submission\n",
        "- Some tips for improving your score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdPjBTFdkI7t"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IWFJK2h22yc"
      },
      "outputs": [],
      "source": [
        "# dataframe and plotting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# machine learning\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SyQadB9iY8R"
      },
      "source": [
        "### 1. Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQCvC_XjDzyH"
      },
      "outputs": [],
      "source": [
        "# Load files into a pandas dataframe\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "# ss = pd.read_csv('SampleSubmission.csv')\n",
        "variables = pd.read_csv('VariableDefinitions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6xUUoJMaRrc",
        "outputId": "37fa6d78-0297-4468-85e2-68e658ab6d7b"
      },
      "outputs": [],
      "source": [
        "# Let’s observe the shape of our datasets.\n",
        "print('train data shape :', train.shape)\n",
        "print('test data shape :', test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22JNEkmQbMvR"
      },
      "source": [
        "The above output shows the number of rows and columns for train and test dataset. We have 13 variables in the train dataset, 12 independent variables and 1 dependent variable. In the test dataset, we have 12 independent variables.\n",
        "\n",
        "We can observe the first five rows from our data set by using the head() method from the pandas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "0p3FUgRYasI-",
        "outputId": "74d606e0-a4d3-47a7-c1f2-249bff16639e"
      },
      "outputs": [],
      "source": [
        "# inspect train data\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1T_QbjNj0Dq",
        "outputId": "4f65aae1-c7d4-4546-ce33-15d40dd45585"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print('missing values:', train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID5N0vHTj3-F"
      },
      "source": [
        "We don't have missing data in our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "QW4mlprHj-Ir",
        "outputId": "6089fa03-6eb3-4117-f694-b4b901bd57c0"
      },
      "outputs": [],
      "source": [
        "# Explore Target distribution \n",
        "sns.catplot(x=\"bank_account\", kind=\"count\", data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0sPGiOnbTnh"
      },
      "source": [
        "It is important to understand the meaning of each feature so you can really understand the dataset. You can read the VariableDefinition.csv file to understand the meaning of each variable presented in the dataset.\n",
        "\n",
        "The SampleSubmission.csv gives us an example of how our submission file should look. This file will contain the uniqueid column combined with the country name from the Test.csv file and the target we predict with our model. Once we have created this file, we will submit it to the competition page and obtain a position on the leaderboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dPjsaC0_a0qY",
        "outputId": "999100c2-5e84-42df-a3d9-4bbf99a9dd4a"
      },
      "outputs": [],
      "source": [
        "# view the submission file\n",
        "ss.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCP0H7tSicUU"
      },
      "source": [
        "### 2. Understand the dataset\n",
        "We can get more information about the features presented by using the info() method from pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1otwZnE8bGns",
        "outputId": "d1761519-086f-4ffc-9f2e-e968611fe91c"
      },
      "outputs": [],
      "source": [
        " #show some information about the dataset\n",
        " print(train.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UOB8QG_bi_B"
      },
      "source": [
        "The output shows the list of variables/features, sizes, if it contains missing values and data type for each variable. From the dataset, we don’t have any missing values and we have 3 features of integer data type and 10 features of the object data type.\n",
        "\n",
        "If you want to learn how to handle missing data in your dataset, we recommend you read How to [Handle Missing Data with Python](https://machinelearningmastery.com/handle-missing-data-python/) by Jason Brownlee.\n",
        "\n",
        "We won’t go further on understanding the dataset because Davis has already published an article about exploratory data analysis (EDA) with the financial Inclusion in Africa dataset. You can read and download the notebook for EDA in the link below.\n",
        "\n",
        "[Why you need to explore your data and how you can start](https://https://medium.com/analytics-vidhya/why-you-need-to-explore-your-data-how-you-can-start-13de6f29c8c1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "8D-b3rTKAbmq",
        "outputId": "67891a87-702a-419a-8fd5-0f74fed96c36"
      },
      "outputs": [],
      "source": [
        "# Let's view the variables\n",
        "variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDu51_TBceft",
        "outputId": "bc1a4302-34e9-4107-8a69-00d59e1f77d4"
      },
      "outputs": [],
      "source": [
        "#import preprocessing module\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Convert target label to numerical Data\n",
        "le = LabelEncoder()\n",
        "train['bank_account'] = le.fit_transform(train['bank_account'])\n",
        "\n",
        "#Separate training features from target\n",
        "X_train = train.drop(['bank_account'], axis=1)\n",
        "y_train = train['bank_account']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# save the label encoder\n",
        "import pickle\n",
        "filename = '../savedModel/label_encoder.sav'\n",
        "pickle.dump(le, open(filename, 'wb'))\n",
        "\n",
        "# save the minmax scaler\n",
        "filename = '../savedModel/minmax_scaler.sav'\n",
        "pickle.dump(MinMaxScaler, open(filename, 'wb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dance7rnc2a1"
      },
      "source": [
        "The target values have been transformed into numerical datatypes, **1** represents **‘Yes’** and **0** represents **‘No’**.\n",
        "\n",
        "We have created a simple preprocessing function to:\n",
        "\n",
        "*   Handle conversion of data types\n",
        "*   Convert categorical features to numerical features by using [One-hot Encoder and Label Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)\n",
        "*   Drop uniqueid variable\n",
        "*   Perform [feature scaling](https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9).\n",
        "\n",
        "The processing function will be used for both train and test independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5nLXAW5hdYOk"
      },
      "outputs": [],
      "source": [
        "# function to preprocess our data from train models\n",
        "def preprocessing_data(data):\n",
        "\n",
        "    # Convert the following numerical labels from integer to float\n",
        "    float_array = data[[\"household_size\", \"age_of_respondent\", \"year\"]].values.astype(float)\n",
        "\n",
        "    # categorical features to be converted to One Hot Encoding\n",
        "    categ = [\n",
        "        \"relationship_with_head\",\n",
        "        \"marital_status\",\n",
        "        \"education_level\",\n",
        "        \"job_type\",\n",
        "        \"country\"\n",
        "        ]\n",
        "\n",
        "    # One Hot Encoding conversion\n",
        "    data = pd.get_dummies(data, prefix_sep=\"_\", columns=categ)\n",
        "\n",
        "    # Label Encoder conversion\n",
        "    data[\"location_type\"] = le.fit_transform(data[\"location_type\"])\n",
        "    data[\"cellphone_access\"] = le.fit_transform(data[\"cellphone_access\"])\n",
        "    data[\"gender_of_respondent\"] = le.fit_transform(data[\"gender_of_respondent\"])\n",
        "\n",
        "    # drop unique_id column\n",
        "    data = data.drop([\"uniqueid\"], axis=1)\n",
        "\n",
        "    # scale our data into range of 0 and 1\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ecEbBCgdeKC"
      },
      "source": [
        "Preprocess both train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNdoBGhgdYdQ"
      },
      "outputs": [],
      "source": [
        "# preprocess the train data \n",
        "processed_train = preprocessing_data(X_train)\n",
        "processed_test = preprocessing_data(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxD8v8T6dhxp"
      },
      "source": [
        "Observe the first row in the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZxvcf4DdYld",
        "outputId": "a9d357ef-a139-4a83-a081-60c21e912bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.        0.        1.        0.1       0.0952381 0.        0.\n",
            "  0.        0.        0.        0.        1.        0.        0.\n",
            "  1.        0.        0.        0.        0.        0.        1.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        1.        1.        0.\n",
            "  0.        0.       ]]\n"
          ]
        }
      ],
      "source": [
        "# the first train row\n",
        "print(processed_train[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X7hwM0NdlVB"
      },
      "source": [
        "Observe the shape of the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwZVZ4cGdmTP",
        "outputId": "15e18896-281a-4dcc-9b07-b85da0757322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(23524, 37)\n"
          ]
        }
      ],
      "source": [
        "# shape of the processed train set\n",
        "print(processed_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwHinDPjds1S"
      },
      "source": [
        "Now we have more independent variables than before (37 variables). This doesn’t mean all these variables are important to train our model. You need to select only important features that can increase the performance of the model. But we will not apply any feature selection technique in this article; if you want to learn and know more about feature selection techniques, we recommend you read the following articles:\n",
        "\n",
        "\n",
        "*    [Introduction to Feature Selection methods with an example (or how to select the right variables?)](https://https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
        "*   [The 5 Feature Selection Algorithms every Data Scientist should know](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2)\n",
        "*   [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
        "*   [Feature Selection Techniques in Machine Learning with Python](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmPQteHwk4hS"
      },
      "source": [
        "###4. Model Building and Experiments\n",
        "A portion of the train data set will be used to evaluate our models and find the best one that performs well before using it in the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MSEu48clFhH"
      },
      "outputs": [],
      "source": [
        "import sklearn.model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HXDwTMtlB8-"
      },
      "outputs": [],
      "source": [
        "# Split train_data\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "X_Train, X_Val, y_Train, y_val = train_test_split(processed_train, y_train, stratify = y_train, test_size = 0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ZVFNKRlmfg"
      },
      "source": [
        "Only 10% of the train dataset will be used for evaluating the models. The parameter stratify = y_train will ensure an equal balance of values from both classes (‘yes’ and ‘no’) for both train and validation set.\n",
        "\n",
        "There are many models to choose from such as\n",
        "\n",
        "*   [K Nearest Neighbors](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)\n",
        "*   [Logistic Regression](https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/)\n",
        "*   [Random Forest](https://www.datacamp.com/community/tutorials/random-forests-classifier-python)\n",
        "\n",
        "\n",
        "We will start by training these models using the train set after splitting our train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_Train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt1ZHUbwmY_t",
        "outputId": "f277a505-89fd-4a23-a3cb-40ef32664744"
      },
      "outputs": [],
      "source": [
        "#import classifier algorithm here\n",
        "# Create a LightGBM model\n",
        "lgb_model = LGBMClassifier()\n",
        "\n",
        "# Fitting the model\n",
        "lgb_model.fit(X_Train, y_Train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lgb_model.predict(X_Val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewpi1qC8mmwM"
      },
      "source": [
        "The evaluation metric for this challenge will be the percentage of survey respondents for whom you predict the binary 'bank account' classification incorrectly.\n",
        "\n",
        "This means the **lower** the incorrect percentage we get, the better the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_DtQ0CpmZCT",
        "outputId": "7f20aa72-8af2-4e0c-ceaf-94aefdf7f73a"
      },
      "outputs": [],
      "source": [
        "# import evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# evaluate the model\n",
        "\n",
        "# Get error rate\n",
        "print(\"Error rate of XGB classifier: \", 1 - accuracy_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvkSRJNUqMCq"
      },
      "outputs": [],
      "source": [
        "# Get the predicted result for the test Data\n",
        "test.bank_account = lgb_model.predict(processed_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmzWR6qnqQlG"
      },
      "source": [
        "Then we create a submission file according to the instruction provided in the SubmissionFile.csv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELYl_OMUqRhR"
      },
      "outputs": [],
      "source": [
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame(\n",
        "    {\n",
        "        \"uniqueid\": test[\"uniqueid\"] + \" x \" + test[\"country\"],\n",
        "        \"bank_account\": test.bank_account\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWsK-dHOqcIB"
      },
      "source": [
        "Let’s observe the sample results from our submission DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VHhKw7ceqdD2",
        "outputId": "90eb4eab-a985-485a-98df-5f8aa8a19851"
      },
      "outputs": [],
      "source": [
        "#show the five sample\n",
        "submission.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcOeN6Nkqkkw"
      },
      "source": [
        "Save results in the CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FzEYLNpDqjL-",
        "outputId": "1d37b510-2845-4ada-82d4-aeff7c8b1973"
      },
      "outputs": [],
      "source": [
        "# Create submission csv file csv file\n",
        "submission.to_csv('light_gbm_submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model for future inference\n",
        "import joblib\n",
        "joblib.dump(lgb_model, '../savedModel/light_gbm_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# required data for inference\n",
        "# ['country', 'year', 'uniqueid', 'location_type', 'cellphone_access', household_size', 'age_of_respondent', 'gender_of_respondent', 'relationship_with_head', 'marital_status', 'education_level', 'job_type']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_user_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess user-provided data for model inference.\n",
        "\n",
        "    Args:\n",
        "        data (dict): User-provided data as a dictionary containing feature values.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Preprocessed data as a NumPy array suitable for model input.\n",
        "    \"\"\"\n",
        "    # Convert numerical features to float if necessary\n",
        "    numerical_features = [\"household_size\", \"age_of_respondent\", \"year\"]\n",
        "    for feature in numerical_features:\n",
        "        if feature in data:\n",
        "            data[feature] = float(data[feature])\n",
        "\n",
        "\n",
        "    # Categorical features for One-Hot Encoding\n",
        "    categorical_features = [\n",
        "        \"relationship_with_head\",\n",
        "        \"marital_status\",\n",
        "        \"education_level\",\n",
        "        \"job_type\",\n",
        "        \"country\"\n",
        "    ]\n",
        "\n",
        "    # One-Hot Encoding conversion\n",
        "    data = pd.DataFrame(data).get_dummies(prefix_sep=\"_\", columns=categorical_features)\n",
        "\n",
        "    # Load the saved LabelEncoder\n",
        "    le = joblib.load(\"../savedModel/label_encoder.pkl\")\n",
        "\n",
        "    # Apply Label Encoder for specific features (if applicable)\n",
        "    for feature in [\"location_type\", \"cellphone_access\", \"gender_of_respondent\"]:\n",
        "        if feature in data:\n",
        "            data[feature] = le.transform([data[feature]])  # Ensure correct shape\n",
        "\n",
        "    # Drop unnecessary columns (e.g., \"uniqueid\")\n",
        "    data = data.drop(columns=[\"uniqueid\"], axis=1)\n",
        "\n",
        "    # Load the saved MinMaxScaler\n",
        "    scaler = joblib.load(\"../savedModel/minmax_scaler.pkl\")\n",
        "\n",
        "    # Scale data using the MinMaxScaler\n",
        "    data = scaler.transform(data)\n",
        "\n",
        "    # Convert DataFrame back to NumPy array for model input\n",
        "    return data.values\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "StarterNotebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
